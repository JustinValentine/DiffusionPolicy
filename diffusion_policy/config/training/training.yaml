
device: "cuda:0"
seed: 42
debug: False
resume: True
# optimization
lr_scheduler: cosine
lr_warmup_steps: 500 #500
num_epochs: 10000
gradient_accumulate_every: 1
# EMA destroys performance when used with BatchNorm
# replace BatchNorm with GroupNorm.
use_ema: True
freeze_encoder: False
# training loop control
# in epochs
rollout_every: 25
checkpoint_every: 250
val_every: 1
sample_every: 5
# steps per epoch
max_train_steps: null
max_val_steps: null
# misc
tqdm_interval_sec: 1.0
